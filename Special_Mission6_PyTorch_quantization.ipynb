{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Special_Mission6_PyTorch_quantization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"140d547971b745388688b66d8bcbe9b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_679bc713f2fe4a098dee7d581489abaa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f86aced1c09744f3a814558b7f57f737","IPY_MODEL_41e0f20884e54f93a4d47c8057554198"]}},"679bc713f2fe4a098dee7d581489abaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f86aced1c09744f3a814558b7f57f737":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7ceef3a6e932414cb5eef42c5da083de","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_62d141b2e3214752a1dbc36ca988b647"}},"41e0f20884e54f93a4d47c8057554198":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73ba173e619f478597fb22b8454537b3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [07:23&lt;00:00, 384025.12it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e23cf6685f1348a8a12231b7085f2a80"}},"7ceef3a6e932414cb5eef42c5da083de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"62d141b2e3214752a1dbc36ca988b647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73ba173e619f478597fb22b8454537b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e23cf6685f1348a8a12231b7085f2a80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f26cb0b118c4f3f92c3578e468faa6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2e41fda2149240fdaae4121a196f23d1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_317b9bb873d04842b4d51592ec7c9ecc","IPY_MODEL_47b1924cbed44aa7abd8c4ccba4ef8d9"]}},"2e41fda2149240fdaae4121a196f23d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"317b9bb873d04842b4d51592ec7c9ecc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_54e3bbf1469d4292972289037ff27738","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7df08f731157469f84cc178508208167"}},"47b1924cbed44aa7abd8c4ccba4ef8d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ae129845a014c3bb55536fad7af31ad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 69.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11be2fa0022e4d259264298926859c82"}},"54e3bbf1469d4292972289037ff27738":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7df08f731157469f84cc178508208167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ae129845a014c3bb55536fad7af31ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11be2fa0022e4d259264298926859c82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"3KluRK1iJBde"},"source":["import os\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_alfvcHNpTS"},"source":["Dataset 준비 (CIFAR10)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["140d547971b745388688b66d8bcbe9b5","679bc713f2fe4a098dee7d581489abaa","f86aced1c09744f3a814558b7f57f737","41e0f20884e54f93a4d47c8057554198","7ceef3a6e932414cb5eef42c5da083de","62d141b2e3214752a1dbc36ca988b647","73ba173e619f478597fb22b8454537b3","e23cf6685f1348a8a12231b7085f2a80"]},"id":"3gWcxhoAMtkP","outputId":"bb14da42-580c-4aaa-b78d-791eb9cd362c"},"source":["batch_size = 64\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",")\n","\n","trainset = datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"140d547971b745388688b66d8bcbe9b5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-8B51ao3MoBP"},"source":["## 1. Baselline Model 만들기 (Fp32)\n"]},{"cell_type":"markdown","metadata":{"id":"kk203gJtUM7L"},"source":["### 1.1 Model 정의\n","\n","### * Misssion\n","**자신만의 모델을 만들어 같은 과정을 반복하며 quantization을 효과를 비교해봅시다.**"]},{"cell_type":"code","metadata":{"id":"rqGxnjnzMpQn"},"source":["# quantize 가능한 단순한 floating point Model을 정의합니다.\n","class ConvBNReLU(nn.Sequential):\n","    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n","        padding = (kernel_size - 1) // 2\n","        super(ConvBNReLU, self).__init__(\n","            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n","            nn.BatchNorm2d(out_planes, momentum=0.1),\n","            # Replace with ReLU\n","            nn.ReLU(inplace=False)\n","        )\n","\n","class Model(torch.nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # QuantStub을 이용해 floating point에서 quantized tensor로 변환합니다.\n","        self.quant = torch.quantization.QuantStub()\n","        \n","        # Conv2d (in channel, out channel, kernel size)\n","        self.convbnrelu1 = ConvBNReLU(3, 32, 3, 2)\n","        self.convbnrelu2 = ConvBNReLU(32, 64, 3, 1)\n","        self.maxpool1 = nn.MaxPool2d(2)\n","        self.convbnrelu3 = ConvBNReLU(64, 128, 3, 1)\n","        self.maxpool2 = nn.MaxPool2d(2)\n","        self.convbnrelu4 = ConvBNReLU(128, 256, 3, 1)\n","        self.flatten = torch.nn.Flatten()\n","        \n","        # Image size : 32 x 32\n","        self.linear = torch.nn.Linear(256, 10)\n","\n","        # DeQuantStub을 이용해 quantized tensor에서 floating point로 변환합니다.\n","        self.dequant = torch.quantization.DeQuantStub()\n","    \n","    def forward(self, x):\n","        x = self.quant(x)\n","\n","        # |x| = (batch_size, 3, 32, 32)\n","        x = self.convbnrelu1(x)\n","        x = self.convbnrelu2(x)\n","        x = self.maxpool1(x)\n","        x = self.convbnrelu3(x)\n","        x = self.maxpool2(x)\n","        x = self.convbnrelu4(x)\n","\n","        x = nn.functional.adaptive_avg_pool2d(x, 1)\n","\n","        # |x| = (batch_size, 1, 32, 32)\n","        x = self.flatten(x)\n","\n","        # |x| = (batch_size, 3*32*32)\n","        x = self.linear(x)\n","\n","        # |x| = (batch_size, 10)\n","        x = self.dequant(x)\n","        return x\n","\n","    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n","    # This operation does not change the numerics\n","    def fuse_model(self):\n","        for m in self.modules():\n","            if type(m) == ConvBNReLU:\n","                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n","\n","def print_size_of_model(model):\n","    model_path = \"./model.p\"\n","    torch.save(model.state_dict(), model_path)\n","    print('size(mb) : ', os.path.getsize(model_path) / 1e6)\n","    os.remove(model_path)\n","\n","@torch.no_grad()\n","def test_model(model, testloader, device, half=False):\n","    correct = 0\n","    total = 0\n","    model = model.to(device)\n","    start_time = time.time()\n","    for data in testloader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        if half:\n","          images = images.half()\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print(f'Accuracy of the network on the 10000 test images: {round(100.0 * correct / total, 2)}%')\n","    print(f'Elpased time: {round(time.time() - start_time, 3)}s, on {device}')\n","    \n","def load_model(model_file):\n","    model = Model()\n","    state_dict = torch.load(model_file)\n","    model.load_state_dict(state_dict)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Je1K9tnmNw-Y"},"source":["### 1.2 Baseline 모델 학습"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02V3l_X4Mtzv","outputId":"0e47077a-72f9-45b2-8ee5-bdc8c5607991"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","epochs = 20\n","saved_model_dir = 'data'\n","float_model_file = 'pretrained_float.pth'\n","\n","model = Model()\n","model.train()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())\n","model = model.to(device)\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for i, data in enumerate(trainloader, 0):\n","\n","    inputs, labels = data\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    \n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","  print('[%d, %5d] loss: %.3f' %\n","        (epoch + 1, i + 1, running_loss / len(trainloader)))\n","  test_model(model=model, testloader=testloader, device=device)\n","\n","print('Finished Training')\n","\n","# save model\n","fp32_path = os.path.join(saved_model_dir, float_model_file)\n","torch.save(model.state_dict(), fp32_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1,   782] loss: 1.220\n","Accuracy of the network on the 10000 test images: 65.79%\n","Elpased time: 1.734s, on cuda\n","[2,   782] loss: 0.850\n","Accuracy of the network on the 10000 test images: 70.82%\n","Elpased time: 1.755s, on cuda\n","[3,   782] loss: 0.703\n","Accuracy of the network on the 10000 test images: 73.72%\n","Elpased time: 1.697s, on cuda\n","[4,   782] loss: 0.595\n","Accuracy of the network on the 10000 test images: 75.21%\n","Elpased time: 1.7s, on cuda\n","[5,   782] loss: 0.514\n","Accuracy of the network on the 10000 test images: 76.38%\n","Elpased time: 1.732s, on cuda\n","[6,   782] loss: 0.445\n","Accuracy of the network on the 10000 test images: 76.76%\n","Elpased time: 1.695s, on cuda\n","[7,   782] loss: 0.385\n","Accuracy of the network on the 10000 test images: 77.36%\n","Elpased time: 1.72s, on cuda\n","[8,   782] loss: 0.328\n","Accuracy of the network on the 10000 test images: 77.61%\n","Elpased time: 1.699s, on cuda\n","[9,   782] loss: 0.282\n","Accuracy of the network on the 10000 test images: 77.26%\n","Elpased time: 1.716s, on cuda\n","[10,   782] loss: 0.237\n","Accuracy of the network on the 10000 test images: 77.37%\n","Elpased time: 1.726s, on cuda\n","[11,   782] loss: 0.209\n","Accuracy of the network on the 10000 test images: 76.1%\n","Elpased time: 1.7s, on cuda\n","[12,   782] loss: 0.179\n","Accuracy of the network on the 10000 test images: 77.12%\n","Elpased time: 1.789s, on cuda\n","[13,   782] loss: 0.156\n","Accuracy of the network on the 10000 test images: 77.35%\n","Elpased time: 1.692s, on cuda\n","[14,   782] loss: 0.133\n","Accuracy of the network on the 10000 test images: 76.09%\n","Elpased time: 1.704s, on cuda\n","[15,   782] loss: 0.119\n","Accuracy of the network on the 10000 test images: 77.71%\n","Elpased time: 1.686s, on cuda\n","[16,   782] loss: 0.108\n","Accuracy of the network on the 10000 test images: 76.19%\n","Elpased time: 1.716s, on cuda\n","[17,   782] loss: 0.100\n","Accuracy of the network on the 10000 test images: 77.04%\n","Elpased time: 1.683s, on cuda\n","[18,   782] loss: 0.090\n","Accuracy of the network on the 10000 test images: 77.05%\n","Elpased time: 1.692s, on cuda\n","[19,   782] loss: 0.080\n","Accuracy of the network on the 10000 test images: 76.29%\n","Elpased time: 1.719s, on cuda\n","[20,   782] loss: 0.082\n","Accuracy of the network on the 10000 test images: 76.65%\n","Elpased time: 1.746s, on cuda\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YOwuJn0tMA1v"},"source":["## 2. Fp16 quantization"]},{"cell_type":"markdown","metadata":{"id":"AYSU5vGmM7aQ"},"source":["### 2.1 Fp16 quantization"]},{"cell_type":"code","metadata":{"id":"THHgyuZ_MyLX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"197ec1c6-7ad0-4eef-e812-8b87830f2e63"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = load_model(fp32_path)\n","model.eval()\n","print(\"[fp32]\")\n","print_size_of_model(model)\n","test_model(model=model,\n","           testloader=testloader,\n","           device=device,\n","           half=False)\n","model = load_model(fp32_path)\n","model.eval()\n","fp16_model = model.half()\n","print(\"[fp16]\")\n","print_size_of_model(fp16_model)\n","test_model(model=fp16_model,\n","           testloader=testloader,\n","           device=device,\n","           half=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fp32]\n","size(mb) :  1.578521\n","Accuracy of the network on the 10000 test images: 77.26%\n","Elpased time: 1.695s, on cuda\n","[fp16]\n","size(mb) :  0.793689\n","Accuracy of the network on the 10000 test images: 77.26%\n","Elpased time: 1.694s, on cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AU7MQVz1RRoE"},"source":["## 3. Post Training Quantization (Static Quantization)"]},{"cell_type":"markdown","metadata":{"id":"hyJ67UyDRV2z"},"source":["* Post Training Quantization(이하 PTQ)은 model의 weights와 activations를 fp32 -> qint8로 quantize 합니다. \n","* PTQ는 activations를 이전 레이어에 fuse 시킵니다.\n","* Quantization 할 때, activation의 optimal quantization parameter를 찾기 위해 representative dataset이 필요합니다."]},{"cell_type":"markdown","metadata":{"id":"Apo2UJIFRYjV"},"source":["* model 인스턴스를 만들고, static quantization을 위해 eval mode로 세팅합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJsh_-q7RdAz","outputId":"a15cbf2d-4f67-4a71-bfe3-3420c5473d9a"},"source":["model = load_model(fp32_path)\n","model.eval()\n","model = model.to('cpu')\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model(\n","  (quant): QuantStub()\n","  (convbnrelu1): ConvBNReLU(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (convbnrelu2): ConvBNReLU(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (convbnrelu3): ConvBNReLU(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (convbnrelu4): ConvBNReLU(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear): Linear(in_features=256, out_features=10, bias=True)\n","  (dequant): DeQuantStub()\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HbDl7Ob2RjfU"},"source":["* pytorch에서는 사용하는 cpu의 사양에 따라  두 종류의 quantization backend를 지원합니다.\n","* x86 CPUs 경우 'fbgemm'\n","* ARM CPUs 경우 'qnnpack'\n"]},{"cell_type":"code","metadata":{"id":"hN-4ds6uRfuk"},"source":["backend = \"fbgemm\"\n","model.qconfig = torch.quantization.get_default_qconfig(backend)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96JI-lAbRo6r"},"source":["* activations를 이전 레이어에 fuse 시킵니다\n","* calibration을 위해 prepare된 model에 representative 한 dataset을 입력합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDWRY2qBRhKE","outputId":"dfd42091-7c66-4855-d7a6-a2f9f0f1b5f2"},"source":["model.fuse_model()\n","model_prepared = torch.quantization.prepare(model)\n","\n","# calibration with traindata\n","test_model(model_prepared, trainloader, 'cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/quantization/observer.py:123: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  reduce_range will be deprecated in a future release of PyTorch.\"\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 98.39%\n","Elpased time: 46.7s, on cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V4cio2fRRsMr"},"source":["* 모델을 convert 하여 quantization 된 model을 얻습니다.\n","* model의 크기를 출력하여 비교합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nf4KSdSRqU7","outputId":"22300f30-e5ba-40c2-a6e0-aee025a58d85"},"source":["model_int8 = torch.quantization.convert(model_prepared)\n","\n","model = load_model(fp32_path)\n","model.eval()\n","print(\"[fp32]\")\n","print_size_of_model(model)\n","test_model(model, testloader, 'cpu')\n","\n","print(\"[int8(ptq)]\")\n","print_size_of_model(model_int8)\n","test_model(model_int8, testloader, 'cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fp32]\n","size(mb) :  1.578521\n","Accuracy of the network on the 10000 test images: 77.26%\n","Elpased time: 7.367s, on cpu\n","[int8(ptq)]\n","size(mb) :  0.410703\n","Accuracy of the network on the 10000 test images: 77.4%\n","Elpased time: 4.433s, on cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UCD4DRG_RvUE"},"source":["## 4. Quantization Aware Training (QAT)\n","* Quantization Aware Trainig(QAT)는 위에서 설명한 방법과 다르게 training 과정에서 quantization error를  모델링하여 quantization 합니다.\n","* PTQ와 비슷하게 모델을 fuse 시키고 prepare 합니다."]},{"cell_type":"code","metadata":{"id":"y9qW9liCRtir"},"source":["model = load_model(fp32_path)\n","model = model.to('cpu')\n","model.train()\n","\n","backend = \"fbgemm\"\n","model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n","model.fuse_model()\n","qat_model = torch.quantization.prepare_qat(model, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvWNXebfRxV0"},"source":["device = \"cpu\"\n","# train\n","for epoch in range(8):\n","  running_loss = 0.0\n","  if epoch > 3:\n","      # Freeze quantizer parameters\n","      qat_model.apply(torch.quantization.disable_observer)\n","  if epoch > 2:\n","      # Freeze batch norm mean and variance estimates\n","      qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n","  for i, data in enumerate(trainloader, 0):\n","    inputs, labels = data\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = qat_model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss += loss.item()\n","  quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n","  quantized_model.eval()\n","  print('[%d, %5d] loss: %.3f' %\n","        (epoch + 1, i + 1, running_loss / len(trainloader)))\n","  test_model(model=quantized_model, testloader=testloader, device=device)\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"byevPWwjR0tE"},"source":["* quantization 이전 모델과 이후 모델의 크기를 확인합니다."]},{"cell_type":"code","metadata":{"id":"e44uWDxBRzE8"},"source":["qat_int8 = torch.quantization.convert(qat_model.eval())\n","\n","model = load_model(fp32_path)\n","model.eval()\n","print(\"[fp32]\")\n","print_size_of_model(model)\n","test_model(model, testloader, 'cpu')\n","\n","print(\"[int8(qat)]\")\n","print_size_of_model(qat_int8)\n","test_model(qat_int8, testloader, 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zuvl9PtnVCJ7"},"source":["## 5. Mixed precision\n","* model.half()를 사용하여 model의 dtype을 fp32 -> fp16으로 변환합니다.\n","* quantization을 위해 model을 eval mode로 설정합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8JDdjIVVFtI","outputId":"dc4fb169-4e0b-4c46-a494-abf3dc6b8b90"},"source":["model_fp32 = Model()\n","model_fp32.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (quant): QuantStub()\n","  (convbnrelu1): ConvBNReLU(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (convbnrelu2): ConvBNReLU(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (convbnrelu3): ConvBNReLU(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (convbnrelu4): ConvBNReLU(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear): Linear(in_features=256, out_features=10, bias=True)\n","  (dequant): DeQuantStub()\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8t79-zBdVFv4","outputId":"c9bcd498-6f6a-4b48-ce41-e669f49bf254"},"source":["print(\"[fp32]\")\n","print_size_of_model(model_fp32)\n","\n","\n","print(\"[fp16\")\n","model_fp16 = model_fp32.half()\n","print_size_of_model(model_fp16)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fp32]\n","size(mb) :  1.578521\n","[fp16\n","size(mb) :  0.793689\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wW0SuGZ6V4WU"},"source":["## 6. Conv-bn fuse \n","Code mostly borrowed from https://learnml.today/speeding-up-model-with-fusing-batch-normalization-and-convolution-3"]},{"cell_type":"code","metadata":{"id":"ET7aE2GzVxXQ"},"source":["import torch\n","import torchvision\n","\n","def fuse_convbn(conv, bn):\n","    \"\"\"Fuse conv + bn module into single conv.\"\"\"\n","\n","    fused = torch.nn.Conv2d(\n","        conv.in_channels,\n","        conv.out_channels,\n","        kernel_size=conv.kernel_size,\n","        stride=conv.stride,\n","        padding=conv.padding,\n","        bias=True\n","    )\n","\n","    # Setting weights\n","    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n","    # Scaling factor of combined normalize - renormalize\n","    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n","    fused.weight.copy_(torch.mm(w_bn, w_conv).view(fused.weight.size()))\n","    \n","    # Setting bias\n","    if conv.bias is not None:\n","        b_conv = conv.bias\n","    else:\n","        b_conv = torch.zeros(conv.weight.size(0))\n","    # Shifting factor of combined normalize - renormalize\n","    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(\n","                            torch.sqrt(bn.running_var + bn.eps)\n","                        )\n","    fused.bias.copy_(bn.weight.mul(b_conv) + b_bn)\n","\n","    return fused"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyKisWjtWGOZ"},"source":["Check if its equivalent, compare speed"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["5f26cb0b118c4f3f92c3578e468faa6b","2e41fda2149240fdaae4121a196f23d1","317b9bb873d04842b4d51592ec7c9ecc","47b1924cbed44aa7abd8c4ccba4ef8d9","54e3bbf1469d4292972289037ff27738","7df08f731157469f84cc178508208167","0ae129845a014c3bb55536fad7af31ad","11be2fa0022e4d259264298926859c82"]},"id":"ZvO_mKSwVxZV","outputId":"02ce8e62-1165-4724-8552-c7a8ee38ae8e"},"source":["import torch.autograd.profiler as profiler\n","\n","# we need to turn off gradient calculation because we didn't write it\n","torch.set_grad_enabled(False)\n","x = torch.randn(16, 3, 256, 256)\n","resnet18 = torchvision.models.resnet18(pretrained=True)\n","# removing all learning variables, etc\n","resnet18.eval()\n","# detach only single layer\n","conv_bn = torch.nn.Sequential(\n","    resnet18.conv1,\n","    resnet18.bn1\n",")\n","fused_conv = fuse_convbn(conv_bn[0], conv_bn[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f26cb0b118c4f3f92c3578e468faa6b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLIrvqZsWJTJ","outputId":"aa92c15b-7cf3-416b-c63c-ac05de644f60"},"source":["device = torch.device(\"cpu\")\n","x = x.to(device)\n","conv_bn = conv_bn.to(device)\n","with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n","    with profiler.record_function(\"model_inference\"):\n","        f1 = conv_bn.forward(x)\n","print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n","--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                 model_inference         1.99%       2.634ms        99.98%     132.087ms     132.087ms      64.00 Mb     -64.00 Mb             1  \n","                    aten::conv2d         0.01%      11.000us        86.91%     114.826ms     114.826ms      64.00 Mb           0 b             1  \n","               aten::convolution         0.01%      17.192us        86.91%     114.815ms     114.815ms      64.00 Mb           0 b             1  \n","              aten::_convolution         0.03%      41.494us        86.89%     114.797ms     114.797ms      64.00 Mb           0 b             1  \n","        aten::mkldnn_convolution        86.74%     114.590ms        86.86%     114.756ms     114.756ms      64.00 Mb           0 b             1  \n","                aten::batch_norm         0.01%       9.574us        11.07%      14.625ms      14.625ms      64.00 Mb           0 b             1  \n","    aten::_batch_norm_impl_index         0.01%      18.458us        11.06%      14.615ms      14.615ms      64.00 Mb           0 b             1  \n","         aten::native_batch_norm        11.03%      14.573ms        11.04%      14.592ms      14.592ms      64.00 Mb        -512 b             1  \n","                     aten::empty         0.14%     183.506us         0.14%     183.506us      26.215us     128.00 Mb     128.00 Mb             7  \n","                     aten::zeros         0.01%      15.104us         0.02%      27.342us      27.342us           4 b           0 b             1  \n","--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 132.114ms\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dKy8DdqWLf4","outputId":"0dead291-2277-4c58-f8cd-43ca40635fbc"},"source":["x = x.to(device)\n","fused_conv = fused_conv.to(device)\n","with profiler.profile(record_shapes=True, profile_memory=True) as prof:\n","    with profiler.record_function(\"model_inference\"):\n","        f2 = fused_conv.forward(x)\n","print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n","----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","             model_inference         0.10%      93.046us        99.97%      95.145ms      95.145ms      64.00 Mb         -20 b             1  \n","                aten::conv2d         0.01%       9.681us        99.87%      95.050ms      95.050ms      64.00 Mb           0 b             1  \n","           aten::convolution         0.01%      10.625us        99.86%      95.040ms      95.040ms      64.00 Mb           0 b             1  \n","          aten::_convolution         0.02%      19.649us        99.85%      95.029ms      95.029ms      64.00 Mb           0 b             1  \n","    aten::mkldnn_convolution        99.71%      94.897ms        99.83%      95.010ms      95.010ms      64.00 Mb           0 b             1  \n","                 aten::empty         0.12%     117.148us         0.12%     117.148us      39.049us      64.00 Mb      64.00 Mb             3  \n","                 aten::zeros         0.01%      14.272us         0.03%      27.579us      27.579us           4 b           0 b             1  \n","           aten::as_strided_         0.01%       8.745us         0.01%       8.745us       8.745us           0 b           0 b             1  \n","                 aten::zero_         0.00%       2.788us         0.00%       2.788us       2.788us           0 b           0 b             1  \n","----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 95.173ms\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOvlfsQcWNB5","outputId":"79b0ba35-6ec8-4a3f-9c6e-fcdbcf6daa7f"},"source":["d = (f1 - f2).mean().item()\n","print(\"error:\",d)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["error: 9.24077741409901e-12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j9-KLlv-R4D1"},"source":["# Further studies"]},{"cell_type":"markdown","metadata":{"id":"jUx_RH-JR7mk"},"source":["https://tutorials.pytorch.kr/advanced/static_quantization_tutorial.html"]},{"cell_type":"code","metadata":{"id":"gdkWr4vUR56b"},"source":[""],"execution_count":null,"outputs":[]}]}